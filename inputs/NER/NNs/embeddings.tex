\documentclass[citation\_needed]{subfiles}
\begin{document}

Un réseau de neurones est avant tout un objet mathématique utilisant des \emph{tenseurs}. Les tenseurs sont des objets mathématiques permettant de généraliser les scalaires et les vecteurs dans les espaces vectoriels. Un scalaire (un nombre réel) est un tenseur d'ordre 0, un vecteur un tenseur d'ordre 1, une matrice un tenseur d'ordre 2, etc... Pour pouvoir utiliser un réseau de neurones dans le cadre du TAL, son entrée, dans notre cas les tokens, doivent être \emph{représentés} en tenseurs afin qu'un réseau de neurone puisse les interpréter.

Soit V, le vocabulaire des tokens connu, dont la taille est notée $|V|$, on note $V(w)$ l'indice d'un token $w$ dans $V$. La représentation la plus simple d'un token de $V$ pour un réseau de neurones est sous la forme d'une représentation appelée \textit{one-hot}. Il s'agit d'un vecteur de taille $|V|$ où l'ensemble des valeurs est égal à 0, sauf à l'indice $V(w)$, où cette valeur est de 1. Il s'agit de la représentation la plus directe d'un indice dans un ensemble de taille fixe, on peut interpréter une représentation \textit{one-hot} comme la représentation mathématique qui servira à sélectionner un élément correspondant au token w d'indice $V(w)$. On appelle ces représentations des représentations creuses, dans le sens où la majorité des éléments d'un objet représentant un token est nulle.

À cette représentation creuse est opposée une représentation dense. Il s'agit d'un vecteur D, de taille $|D|$, où la majorité des valeurs est non-nulle. Un intérêt des représentations denses est la dimensionalité du vecteur, qui ne dépend plus de la taille du vocabulaire, ce qui permet d'avoir des systèmes plus légers et plus simples d'un point de vue computationel. Un autre est qu'il s'agit d'une représentation dite distribuée, où l'on suppose que les différentes propriétés relatives à un token sont à différents endroits de la représentation. Nous voyons ici l'intérêt d'un point de vue de la compression des données que les représentations denses peuvent avoir, en particulier quand $|D|\ < |V|$. Un autre avantage des représentations denses est qu'il est possible des les comparer entre elles, à l'inverse des représentations creuse. Il est notamment possible de calculer des mesures de similarité entre deux représentations. L'une des mesures les plus utilisées dans le TAL est la similarité cosinus, qui se définit pour deux vecteurs A et B comme suit :

\begin{equation}\label{eq:cosine-similarity}
\cos \theta = \frac{A \cdot B}{\|A\| \times \|B\|} = \frac{\sum_{i=1}^{n}A_{i} * B_{i}}{\sqrt{\sum_{i=1}^{n}A_{i}^{2}} \times \sqrt{\sum_{i=1}^{n}B_{i}^{2}}}
\end{equation}

Où $\cdot$ est le produit scalaire et $\|A\|$ représente la norme du vecteur A. La valeur de la similarité cosinus est définie dans l'intervalle $[-1,1]$, où -1 indiquera des vecteurs opposés, 1 des vecteurs colinéaires.

%Prenons l'analogie d'un jeu de cartes pour illustrer les différences entre une représentation creuse et une dense, ainsi que pourquoi une représentation dense est plus puissante. Supposons que nous avons un jeu de cartes se jouant avec un nombre arbitraire de joueurs, qui ne dépend pas du nombre de cartes dans le jeu. Le jeu peut se jouer de deux façons différentes, la manière creuse, et la manière dense.
%Pour la manière creuse, chaque carte est placée sur le plateau, un joueur pouvant simplement piocher une carte précise en sachant qu'elle est différente de toutes les autres, sans pour autant pouvoir les comparer. Pour la manière dense, l'ensemble des cartes va être entièrement distribuée à l'ensemble des joueurs. On ne compare alors plus des cartes, mais des distributions de cartes au joeurs. Nous avons la possibilité de comparer les distributions en regardant quels joueurs ont les valets, lesquels ont les carreaux ou les faces, etc... Plus ces distributions sont proches, plus nous considérons que les parties sont similaires.

Dans le TAL, ces représentations ne peuvent pas être directement être déduites des tokens, car cela impliquerait de pouvoir les analyser de manière systématique, ce qui n'est pas possible. Des représentations peuvent être en revanche calculées sur un ensemble de tokens connus, ici le vocabulaire $V$. Chaque token a alors une représentation dense lui étant propre, cette dernière se calcule de manière classique. Étant donné un token $w$ et sa représentation creuse $S(w)$, le passage à une représentation dense $E(w)$ se fait via une multiplication matricielle :

\begin{equation}\label{eq:sparse-to-dense}
E(w) = S(w) \times E
\end{equation}

où $E$ est une matrice de paramètres dont les poids seront ajustés à l'apprentissage afin de fournir une représentation des tokens adaptée à la tâche. Ces représentations s'appellent des \textit{embedding} en anglais. Par la suite, nous emploierons «\ représentation\ » pour traduire \textit{embedding}. D'autres traductions ont été proposées, notamment «\ plongement\ ».

Ce type de représentation ne corrige donc pas (entièrement) le problème des tokens inconnus. Les représentations sont calculées en se basant sur des analyses distributionnelles. L'intuition de ce type d'analyse est que «\ les tokens ayant des contextes similaires ont des sens similaires.\ ». Cette approche a l'avantage de fournir des représentations dites distributionnelles très puissantes, particulièrement adaptées à l'analogie. Les représentations des tokens utilisées dans le TAL sont donc distribuées et distributionnelles. Notamment, des équivalences intéressantes ont été montrées, comme $E(Paris)\ -\ E(France)\ +\ E(Italy)\ \approx\ E(Rome)$, que l'on peut lire "Rome est à l'Italie ce que Paris est à la France". Ces représentations souffrent cependant du même problème que les représentations creuses : un token inconnu n'aura pas de représentation. Le logiciel le plus utilisé à l'heure actuelle permettant de calculer des représentations distributionnelles pour des tokens est word2vec \citep{mikolov2014word2vec}.

\end{document}