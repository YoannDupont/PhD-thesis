\documentclass[PhD-Yoann-Dupont.tex]{subfiles}
\begin{document}

%\textcolor{red}{Idée 1 : apprendre à reconnaître les sous-séquences utiles en apprenant des modèles au niveau des caractères. Comparaison entre CRF et Deep learning.}

%\textcolor{red}{Idée 2 : intégrer la "convolution de caractères" dans les traits : générer toutes les séquences de 1..N caractères, virer ceux qui sont trop mauvais et apprendre avec toutes ces infos.}

%\textcolor{blue}{Idée 2 : avec CRFsuite (puisque Wapiti ne le permet pas facilement), j'ai intégré différentes informations : préfixes et suffixes de taille 1..7 et convolution de caractères de taille 1..7. On a déjà de meilleurs résultats qu'avec l'extraction d'affixes (80.82 $>$ 81.26). On peut facilement inclure l'ensemble de tous les préfixes, de tous les suffixes et tous les infixes... Pourquoi se limiter ?}

Nous avons voulu comparer notre méthode de recherche automatique d'affixes pertinents à deux méthodes par apprentissage automatique. La première consiste à utiliser Morfessor afin d'apprendre automatiquement un segmenteur des tokens au niveau de leurs caractères. La seconde se veut encore plus simple : donner l'ensemble des sous-séquences présentes dans un token au CRF. Cela est assez proche d'une autre méthode, plus utilisée dans les réseaux de neurones : la convolution au niveau des caractères. Proche, car un CRF ne peut pas effectuer une convolution à proprement parler, il est en revanche possible de lui fournir l'ensemble des n-grammes trouvés dans un token donné, n n'ayant pas à être fixé en avance. Les différences principales qu'il y a entre cet ensemble de traits et une véritable convolution de caractères et que l'ordre est perdu ainsi que les doublons. Cette génération des infixes permet d'obtenir un avantage par rapport à une convolution classique : la possibilité de recourir à des dictionnaires. Nous pouvons par exemple aisément dire que, pour un token, un de ses préfixes, suffixes ou infixes appartient à un lexique donné, comme par exemple celui des atomes ou des alkanes (ethyl, methyl, etc.). Un autre avantage à utiliser un CRF plutôt qu'un réseau de neurones est la possibilité d'ajouter des features supplémentaires qu'il serait compliqué d'intégrer dans un réseau de neurones : nous pouvons également effectuer des comptages comme l'a fait le vainqueur de la tâche CEM, qui avait compté les différentes classes de caractères (alphabétique, numérique, autres). Dans les expériences que nous avons menées, nous avons trouvé qu'une taille d'au maximum 7 pour les préfixes et suffixes était plus performante, les performances ne s'améliorent pas après et peuvent même se dégrader si l'on prend l'ensemble de tous les préfixes et suffixes d'un token sans taille maximale fixée, le plus grand préfixe ayant alors la taille du token moins un unique caractère.

\end{document}